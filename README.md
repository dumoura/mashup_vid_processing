# Read Me

Script developed in a specific context for scraping, processing and analysis of media data - video mashups.

## Discuss method.
Here it was adopted the methodological model proposed by Navas (2012; 2013; 2015), that revealed relationships and patterns existing in the AMV's audiovisual dataset. 
The use of software such as ImagePlot helped us understand what is typical and what is unique in a given media dataset and to identify common characteristics and similar patterns between them. However, in order to create proper conditions for submitting our corpus to media visualization procedures that could correspond to our goals, one had to add some steps to the proposal originally developed by Manovich (2010; 2011). 
Before moving to what Manovich calls "digital processing" and "visualization" moments, we have prepared all videos selected according to Digital Humanities and Corpus Linguistics procedures, such as the elaboration of controlled vocabulary and part-of-speech tagging. Therefore, we divided the visualization techniques model proposed by Manovich and Navas into three stages: (1) digital processing, (2) tagging, and (3) exploratory data analysis, as can be seen next.

## Digital processing 
First, with the aid of the **moviepy.editor** (see *notebook - 3_Sample*), an open-source media player software, we have defined extraction parameter to provide an equivalent proportion between our dataset, selecting one image/frames of every three frames – or one image/frame of every two frames, if the video had less than three minutes. Furthermore, each frame was named, numbered, grouped, and saved automatically by music video mashups. Then, the set of frames that forms our corpus could be processed digitally using ImagePlot parameters such as luminosity, saturation, and hue. Moreover, the corpus was submitted to the ImageShapes plugin, which evaluates the number of shapes presenting in each image/frame automatically, generating a tab-delimited text file (.txt). At the end of this step, we have compiled all visual parameters processed by ImageMeasure and ImageShapes plugins into a single spreadsheet to further  develop a supervised machine learning model for tagging all frames.

## Tagging visual features
With the assistance of digital asset management software (Adobe Bridge), a significant part of our dataset was tagged manually,  enabling us, when it was needed, to process and analyze the corpus in different sets of images (visualizations). 
Therefore, according to our goals, we defined parameters for tagging the corpus, aiming at how image and sound are combined:

List of tags:
    •	**Special-Effects/Titles**: Title or special effect independent of pre-existing video footage
    •	**Main-track**: Instrumental/main-track
    •	**Lyrics**: Lyrics or track on top of the main track 
    •	**Overlaps**: Footage of both videos combined, or hybrid video with special effects
    •	**Complementary-track**: Complementary track or footage introduced

For example, the image/frame "NellyBeeGs169.jpg" from the video mashup "Rick Astley vs Nirvana Never gonna give/smells like teen spirit MSAH UP!" received the tag "Main-track". And the frame "NellyBeeGs304.jpg" received the tag "Lyrics".

So, we defined a first set of frames (corresponding one frame per second) to be manually tagged, and exported as comma-separated values (CSV). 

Further, the CSV document was compiled with the previous spreadsheet (Figure 1) and used to train a classification machine learning model, developing a classification algorithm (mainly, based on decision tree learning) to tag the totality of our corpus according to previous labeling criteria. 

Next, all classification generated by our ML model was imported, added as tags to all corresponding images/frames and reviewed in Adobe Bridge.

Furthermore, we recorded "actions" in Photoshop to fill automatically all frames with specific transparency and colors, corresponding to tags established earlier (Figure 4). So, according to our research goals, it was possible to filter the corpus by its metadata and so to batch the frames into at least five different sets for digital processing, and analysis of our audiovisual dataset, exploring and visualizing it into different layouts.
 
In that way, we have created two different montage layouts: one matching our color code; and the other, mixing, in transparency, a color with the corresponding frame, which enables the reader to visualize the content better, as we are going to see next.

## Exploratory data analysis
After the previous step, we developed visualizations for an initial exploration of the corpus. In that sense, we adopted the visualization techniques developed by Navas, which have allowed us to create montages that enabled us to reveal patterns existing in the audiovisual dataset.  As Navas and Manovich suggest it (MANOVICH, 2010; 2011; NAVAS, 2012; 2013; 2015), we rely on the media visualization features offered by the ImagePlot software, which distinguishes three direct visualization techniques: montage visualization, synthesized image, and slice visualization. 
We focus on the montage visualization technique involving arranging the frames of a video in a rectangular grid according to its original sequence (Figure 6). One this diagram, each square represents a video's frame: the left video has 20 frames, and the right, 17. It allows the researcher to visualize the formal and content patterns of an entire video in a "direct" and "instantaneous" way. This technique is particularly recommended for comparing different videos at the same time.
 
In our particular case, different montage was created with a grid of images/frames consisting of about 40 x 40,  varying the high depending on the overall amount of video stills. The visualizations generated this way has provided us with a good sense of how image and sound are combined to "complement" each other in music video mashups, reviling a specific aesthetic and expressive patterns. 

 
Based on these visualizations, we could see that the creative approaches vary, but there are basically similarities between them. More generally, we could attest that:
    •	The image supports the music mashup aural experience, reinforcing that the video is a visual expression of the music mashup. 
    •	The footage is selected and edited to enhance aspects that can put in evidence music mashup aesthetic and expressive values. 
    •	Usually, it appropriates and redefines the song videos language/aesthetic, already present on some sampled material.

As well, we could verify that, through these practices, different visual elements (cuts, effects and/or events in video footage) are sync to match music rhythmic values - as can be seen in "Michael Jackson vs James Brown (mashup by MadMixMustang)," where various visual effects are added to Michael Jackson footage to match with James Brown music beat.

Tutorials:

https://docs.google.com/document/d/1FAZ8JIrY9b-N-W3DYrC5FGfbGlQpCT6WVQQ8-uqtv04/edit?usp=sharing





